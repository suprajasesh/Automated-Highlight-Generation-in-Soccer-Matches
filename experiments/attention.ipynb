{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdcpysVfJoS2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def extract_attention_weights(model, data_loader, event_mapper, device='cuda'):\n",
        "    \"\"\"\n",
        "    Extract attention weights for different event types from the trained model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Dictionary to store attention weights for each event type\n",
        "    event_attention = {}\n",
        "\n",
        "    # Initialize dictionaries for each event type we want to analyze\n",
        "    target_events = ['Goal', 'Foul', 'Corner', 'Yellow card', 'Red card']\n",
        "    for event in target_events:\n",
        "        event_idx = event_mapper.event_to_index(event)\n",
        "        event_attention[event_idx] = []\n",
        "\n",
        "    # Add special handling for cards (combine Yellow and Red)\n",
        "    card_indices = [\n",
        "        event_mapper.event_to_index('Yellow card'),\n",
        "        event_mapper.event_to_index('Red card'),\n",
        "        event_mapper.event_to_index('Yellow->red card')\n",
        "    ]\n",
        "\n",
        "    # Extract attention weights\n",
        "    with torch.no_grad():\n",
        "        for features, labels in tqdm(data_loader, desc=\"Extracting attention weights\"):\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # We need to modify the forward pass to capture attention weights\n",
        "            # This requires modifying the model class to return attention weights\n",
        "\n",
        "            batch_size, seq_len, feat_dim = features.size()\n",
        "\n",
        "            # Apply feature reduction (copied from model's forward method)\n",
        "            x = features.clone()\n",
        "            orig_x = x.clone()\n",
        "            x = x.view(-1, feat_dim)\n",
        "            x = model.feature_reducer(x.view(batch_size, seq_len, -1))\n",
        "\n",
        "            # Multi-scale temporal convolution\n",
        "            x_perm = x.permute(0, 2, 1)\n",
        "\n",
        "            conv1_out = torch.relu(model.conv1(x_perm))\n",
        "            conv2_out = torch.relu(model.conv2(x_perm))\n",
        "            conv3_out = torch.relu(model.conv3(x_perm))\n",
        "\n",
        "            # Concatenate convolutional outputs\n",
        "            conv_combined = torch.cat([conv1_out, conv2_out, conv3_out], dim=1)\n",
        "\n",
        "            # Apply batch normalization\n",
        "            conv_combined = model.bn1(conv_combined)\n",
        "\n",
        "            # Pass through LSTM\n",
        "            lstm_in = conv_combined.permute(0, 2, 1)\n",
        "            lstm_out, _ = model.lstm1(lstm_in)\n",
        "\n",
        "            # Apply batch normalization\n",
        "            lstm_bn = model.bn2(lstm_out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "            # Get attention weights\n",
        "            attn_scores = model.attention(lstm_bn).squeeze(-1)\n",
        "            attn_weights = F.softmax(attn_scores, dim=1)\n",
        "\n",
        "            # Store attention weights by event type\n",
        "            for i in range(batch_size):\n",
        "                label = labels[i].item()\n",
        "\n",
        "                # Only store weights for our target events\n",
        "                if label in event_attention:\n",
        "                    event_attention[label].append(attn_weights[i].cpu().numpy())\n",
        "\n",
        "                # Special handling for cards (combine different card types)\n",
        "                elif label in card_indices:\n",
        "                    # If it's the first card we're seeing, initialize the 'Card' category\n",
        "                    if 'Card' not in event_attention:\n",
        "                        event_attention['Card'] = []\n",
        "                    event_attention['Card'].append(attn_weights[i].cpu().numpy())\n",
        "\n",
        "    # Process the collected attention weights\n",
        "    processed_weights = {}\n",
        "\n",
        "    # Process target events\n",
        "    for event in target_events:\n",
        "        event_idx = event_mapper.event_to_index(event)\n",
        "        if event_idx in event_attention and len(event_attention[event_idx]) > 0:\n",
        "            # Average attention weights across all instances of this event\n",
        "            processed_weights[event] = np.mean(np.vstack(event_attention[event_idx]), axis=0)\n",
        "\n",
        "    # Process combined cards\n",
        "    if 'Card' in event_attention and len(event_attention['Card']) > 0:\n",
        "        processed_weights['Card'] = np.mean(np.vstack(event_attention['Card']), axis=0)\n",
        "\n",
        "    return processed_weights\n",
        "\n",
        "def visualize_attention(attention_weights, window_size=WINDOW_SIZE, fps=FPS):\n",
        "    \"\"\"\n",
        "    Visualize attention weights for different event types\n",
        "    \"\"\"\n",
        "    # Create time steps relative to event occurrence (center of window)\n",
        "    center = window_size // 2\n",
        "    seconds_per_frame = 1.0 / fps\n",
        "    time_steps = np.arange(window_size) - center\n",
        "    time_steps = time_steps * seconds_per_frame\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    # Plot attention distributions\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "    markers = ['o', 's', '^', 'D', 'x']\n",
        "\n",
        "    # Process each event type\n",
        "    for i, (event, weights) in enumerate(attention_weights.items()):\n",
        "        # Ensure we only plot the central region (around the event)\n",
        "        if len(weights) > window_size:\n",
        "            center_idx = len(weights) // 2\n",
        "            start_idx = max(0, center_idx - window_size // 2)\n",
        "            end_idx = min(len(weights), center_idx + window_size // 2)\n",
        "            plot_weights = weights[start_idx:end_idx]\n",
        "            plot_time = time_steps[:len(plot_weights)]\n",
        "        else:\n",
        "            plot_weights = weights\n",
        "            plot_time = time_steps[:len(plot_weights)]\n",
        "\n",
        "        # Plot this event type\n",
        "        ax.plot(plot_time, plot_weights, label=event, linewidth=2,\n",
        "                color=colors[i % len(colors)], marker=markers[i % len(markers)], markersize=8)\n",
        "\n",
        "        # Mark peak attention\n",
        "        peak_idx = np.argmax(plot_weights)\n",
        "        ax.annotate(f'Peak: {plot_weights[peak_idx]:.2f}',\n",
        "                   xy=(plot_time[peak_idx], plot_weights[peak_idx]),\n",
        "                   xytext=(10, (-15 if i % 2 == 0 else 10)),\n",
        "                   textcoords='offset points',\n",
        "                   arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=.2'))\n",
        "\n",
        "    # Add vertical line at event occurrence (t=0)\n",
        "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.7, label='Event Occurrence')\n",
        "    ax.text(0.2, 0.02, 'Event\\nOccurrence', fontsize=9)\n",
        "\n",
        "    # Customize plot\n",
        "    ax.set_xlabel('Time Relative to Event (seconds)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Attention Weight', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Temporal Attention Distribution Across Event Types', fontsize=14, fontweight='bold')\n",
        "    ax.grid(linestyle='--', alpha=0.7)\n",
        "    ax.legend(loc='upper left', frameon=True, framealpha=0.9)\n",
        "\n",
        "    # Add shaded regions for pre-event and post-event\n",
        "    min_time = min(time_steps)\n",
        "    max_time = max(time_steps)\n",
        "    ax.axvspan(min_time, 0, alpha=0.1, color='blue', label='Pre-event Context')\n",
        "    ax.axvspan(0, max_time, alpha=0.1, color='red', label='Post-event Context')\n",
        "    ax.text(min_time + 0.5, 0.23, 'Pre-event Context', fontsize=9, color='darkblue')\n",
        "    ax.text(1, 0.23, 'Post-event Context', fontsize=9, color='darkred')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(SOCCERNET_PATH, 'attention_visualization.png')\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    print(f\"Saved visualization to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "def run_attention_experiment(model, test_loader, event_mapper, device='cuda'):\n",
        "    \"\"\"\n",
        "    Run the complete attention visualization experiment\n",
        "    \"\"\"\n",
        "    print(\"Starting attention visualization experiment...\")\n",
        "\n",
        "    # 1. Extract attention weights from model\n",
        "    attention_weights = extract_attention_weights(model, test_loader, event_mapper, device)\n",
        "    print(f\"Extracted attention weights for {len(attention_weights)} event types\")\n",
        "\n",
        "    # 2. Analyze and post-process the weights if needed\n",
        "    print(\"Processing attention patterns...\")\n",
        "    for event, weights in attention_weights.items():\n",
        "        peak_idx = np.argmax(weights)\n",
        "        peak_time = (peak_idx - WINDOW_SIZE // 2) / FPS\n",
        "        print(f\"{event}: Peak attention at {peak_time:.2f}s relative to event occurrence\")\n",
        "\n",
        "    # 3. Visualize the attention distributions\n",
        "    fig = visualize_attention(attention_weights)\n",
        "\n",
        "    # 4. Save numerical results for future reference\n",
        "    results_path = os.path.join(SOCCERNET_PATH, 'attention_analysis_results.npy')\n",
        "    np.save(results_path, {event: weights for event, weights in attention_weights.items()})\n",
        "\n",
        "    print(\"Experiment completed successfully!\")\n",
        "    return attention_weights\n",
        "\n",
        "# Run the experiment as part of the main function\n",
        "def run_attention_analysis():\n",
        "    # Load the trained model\n",
        "    model_path = '/content/drive/MyDrive/soccernet_event_detection_model.pth'\n",
        "\n",
        "    # Create event mapper\n",
        "    event_mapper = EventMapper()\n",
        "\n",
        "    # Get a subset of test matches\n",
        "    test_matches = get_match_paths()[-10:]  # Take the last 10 matches for testing\n",
        "\n",
        "    # Create test dataset\n",
        "    test_dataset = SoccerNetDataset(test_matches, event_mapper, mode='test')\n",
        "\n",
        "    # Create data loader\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Check if GPU is available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create model\n",
        "    input_dim = 2048  # ResNet features are 2048-dimensional\n",
        "    hidden_dim = 256\n",
        "    num_classes = event_mapper.get_num_classes()\n",
        "    model = EventDetectionModel(input_dim, hidden_dim, num_classes, dropout_rate=0.5)\n",
        "\n",
        "    # Load trained model weights\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    # Run the experiment\n",
        "    attention_weights = run_attention_experiment(model, test_loader, event_mapper, device)\n",
        "\n",
        "    # Clean up\n",
        "    test_dataset.cleanup_cache()\n",
        "\n",
        "    return attention_weights\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the attention analysis experiment\n",
        "    attention_weights = run_attention_analysis()"
      ]
    }
  ]
}