{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ4cxkWcbu5b",
        "outputId": "08d03b8e-ad41-4097-c8fb-94f833f4d4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cpu\n",
            "Loading models...\n",
            "\n",
            "Processing half 1 of 2015-02-21 - 18-00 Chelsea 1 - 1 Burnley...\n",
            "Detecting events...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-10-da20dcfa8a63>:319: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  audio_tensor = torch.tensor(match_audio_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering events...\n",
            "Scoring events...\n",
            "Generating highlight timestamps...\n",
            "Generating highlight timestamps...\n",
            "Evaluating event: Corner at 807.5s, Combined Score: 0.2830, Model Score: 0.4253, Predefined Score: 0.0695\n",
            "Added segment: Corner from 802.5s to 814.5s, Duration: 12.0s, Total Duration: 12.0s\n",
            "Evaluating event: Corner at 879.0s, Combined Score: 0.2829, Model Score: 0.4252, Predefined Score: 0.0695\n",
            "Added segment: Corner from 874.0s to 886.0s, Duration: 12.0s, Total Duration: 24.0s\n",
            "Evaluating event: Direct free-kick at 914.5s, Combined Score: 0.3200, Model Score: 0.4249, Predefined Score: 0.1627\n",
            "Added segment: Direct free-kick from 909.5s to 921.5s, Duration: 12.0s, Total Duration: 36.0s\n",
            "Evaluating event: Corner at 1113.5s, Combined Score: 0.2828, Model Score: 0.4250, Predefined Score: 0.0695\n",
            "Added segment: Corner from 1108.5s to 1120.5s, Duration: 12.0s, Total Duration: 48.0s\n",
            "Evaluating event: Foul at 2530.5s, Combined Score: 0.4688, Model Score: 0.4278, Predefined Score: 0.5302\n",
            "Added segment: Foul from 2525.5s to 2537.5s, Duration: 12.0s, Total Duration: 60.0s\n",
            "Merging overlapping segments...\n",
            "Added non-overlapping segment: Corner from 874.0s to 886.0s\n",
            "Added non-overlapping segment: Direct free-kick from 909.5s to 921.5s\n",
            "Added non-overlapping segment: Corner from 1108.5s to 1120.5s\n",
            "Added non-overlapping segment: Foul from 2525.5s to 2537.5s\n",
            "Generated 5 highlight segments\n",
            "Generated 5 highlight segments\n",
            "\n",
            "Detected 10 events in half 1:\n",
            "1. Foul at 2530.5s, Score: 0.4688\n",
            "2. Direct free-kick at 914.5s, Score: 0.3200\n",
            "3. Corner at 807.5s, Score: 0.2830\n",
            "4. Corner at 879.0s, Score: 0.2829\n",
            "5. Corner at 1113.5s, Score: 0.2828\n",
            "6. Corner at 1119.0s, Score: 0.2828\n",
            "7. Corner at 909.5s, Score: 0.2828\n",
            "8. Corner at 1536.5s, Score: 0.2828\n",
            "9. Clearance at 1566.5s, Score: 0.2601\n",
            "10. Clearance at 1523.5s, Score: 0.2601\n",
            "\n",
            "Creating highlight video for half 1...\n",
            "Attempting to create highlight video: /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4\n",
            "Input video path: /content/drive/MyDrive/soccernet/england_epl/2014-2015/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley/1_224p.mkv\n",
            "Highlight segments: [{'start': 802.5, 'end': 814.5, 'event': 'Corner', 'score': 0.282999134349823}, {'start': 874.0, 'end': 886.0, 'event': 'Corner', 'score': 0.28291155128479}, {'start': 909.5, 'end': 921.5, 'event': 'Direct free-kick', 'score': 0.32002344830513}, {'start': 1108.5, 'end': 1120.5, 'event': 'Corner', 'score': 0.28280574707984923}, {'start': 2525.5, 'end': 2537.5, 'event': 'Foul', 'score': 0.468786964969635}]\n",
            "Loading video file: /content/drive/MyDrive/soccernet/england_epl/2014-2015/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley/1_224p.mkv\n",
            "Video duration: 2700.0s, FPS: 25.0\n",
            "Processing segment: Corner from 802.5s to 814.5s\n",
            "Extracting subclip from 802.5s to 814.5s\n",
            "Added clip of duration 12.0s\n",
            "Processing segment: Corner from 874.0s to 886.0s\n",
            "Extracting subclip from 874.0s to 886.0s\n",
            "Added clip of duration 12.0s\n",
            "Processing segment: Direct free-kick from 909.5s to 921.5s\n",
            "Extracting subclip from 909.5s to 921.5s\n",
            "Added clip of duration 12.0s\n",
            "Processing segment: Corner from 1108.5s to 1120.5s\n",
            "Extracting subclip from 1108.5s to 1120.5s\n",
            "Added clip of duration 12.0s\n",
            "Processing segment: Foul from 2525.5s to 2537.5s\n",
            "Extracting subclip from 2525.5s to 2537.5s\n",
            "Added clip of duration 12.0s\n",
            "Concatenating 5 clips\n",
            "Writing output video to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4\n",
            "Moviepy - Building video /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4.\n",
            "MoviePy - Writing audio in temp-audio.m4a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4\n",
            "Highlight video successfully saved to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4\n",
            "Highlight data saved to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.json\n",
            "\n",
            "Processing half 2 of 2015-02-21 - 18-00 Chelsea 1 - 1 Burnley...\n",
            "Detecting events...\n",
            "Filtering events...\n",
            "Scoring events...\n",
            "Generating highlight timestamps...\n",
            "Generating highlight timestamps...\n",
            "Evaluating event: Offside at 317.5s, Combined Score: 0.4958, Model Score: 0.4254, Predefined Score: 0.6015\n",
            "Added segment: Offside from 312.5s to 324.5s, Duration: 12.0s, Total Duration: 12.0s\n",
            "Evaluating event: Offside at 322.5s, Combined Score: 0.4959, Model Score: 0.4254, Predefined Score: 0.6015\n",
            "Added segment: Offside from 317.5s to 329.5s, Duration: 12.0s, Total Duration: 24.0s\n",
            "Evaluating event: Offside at 395.5s, Combined Score: 0.4957, Model Score: 0.4252, Predefined Score: 0.6015\n",
            "Added segment: Offside from 390.5s to 402.5s, Duration: 12.0s, Total Duration: 36.0s\n",
            "Evaluating event: Goal at 789.5s, Combined Score: 0.6374, Model Score: 0.4249, Predefined Score: 0.9560\n",
            "Added segment: Goal from 784.5s to 796.5s, Duration: 12.0s, Total Duration: 48.0s\n",
            "Evaluating event: Goal at 1678.5s, Combined Score: 0.6396, Model Score: 0.4287, Predefined Score: 0.9560\n",
            "Added segment: Goal from 1673.5s to 1685.5s, Duration: 12.0s, Total Duration: 60.0s\n",
            "Merging overlapping segments...\n",
            "Merged segment: Offside from 312.5s to 329.5s\n",
            "Added non-overlapping segment: Offside from 390.5s to 402.5s\n",
            "Added non-overlapping segment: Goal from 784.5s to 796.5s\n",
            "Added non-overlapping segment: Goal from 1673.5s to 1685.5s\n",
            "Generated 4 highlight segments\n",
            "Generated 4 highlight segments\n",
            "\n",
            "Detected 35 events in half 2:\n",
            "1. Goal at 1678.5s, Score: 0.6396\n",
            "2. Goal at 789.5s, Score: 0.6374\n",
            "3. Offside at 322.5s, Score: 0.4959\n",
            "4. Offside at 317.5s, Score: 0.4958\n",
            "5. Offside at 395.5s, Score: 0.4957\n",
            "6. Offside at 386.5s, Score: 0.4948\n",
            "7. Foul at 2359.5s, Score: 0.4696\n",
            "8. Foul at 2348.0s, Score: 0.4691\n",
            "9. Foul at 2354.0s, Score: 0.4672\n",
            "10. Foul at 1961.0s, Score: 0.4671\n",
            "\n",
            "Creating highlight video for half 2...\n",
            "Attempting to create highlight video: /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4\n",
            "Input video path: /content/drive/MyDrive/soccernet/england_epl/2014-2015/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley/2_224p.mkv\n",
            "Highlight segments: [{'start': 312.5, 'end': 329.5, 'event': 'Offside', 'score': 0.49585737404823305}, {'start': 390.5, 'end': 402.5, 'event': 'Offside', 'score': 0.49573052344322205}, {'start': 784.5, 'end': 796.5, 'event': 'Goal', 'score': 0.6373526035785675}, {'start': 1673.5, 'end': 1685.5, 'event': 'Goal', 'score': 0.6396463572025299}]\n",
            "Loading video file: /content/drive/MyDrive/soccernet/england_epl/2014-2015/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley/2_224p.mkv\n",
            "Video duration: 2700.0s, FPS: 25.0\n",
            "Processing segment: Offside from 312.5s to 329.5s\n",
            "Extracting subclip from 312.5s to 329.5s\n",
            "Added clip of duration 17.0s\n",
            "Processing segment: Offside from 390.5s to 402.5s\n",
            "Extracting subclip from 390.5s to 402.5s\n",
            "Added clip of duration 12.0s\n",
            "Processing segment: Goal from 784.5s to 796.5s\n",
            "Extracting subclip from 784.5s to 796.5s\n",
            "Added clip of duration 12.0s\n",
            "Processing segment: Goal from 1673.5s to 1685.5s\n",
            "Extracting subclip from 1673.5s to 1685.5s\n",
            "Added clip of duration 12.0s\n",
            "Concatenating 4 clips\n",
            "Writing output video to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4\n",
            "Moviepy - Building video /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4.\n",
            "MoviePy - Writing audio in temp-audio.m4a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4\n",
            "Highlight video successfully saved to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4\n",
            "Highlight data saved to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.json\n",
            "Attempting to combine highlight videos into: /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_combined_highlights.mp4\n",
            "Half 1 video: /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4\n",
            "Half 2 video: /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4\n",
            "Loading Half 1 video: /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half1_highlights.mp4\n",
            "Half 1 clip loaded: Duration 60.0s\n",
            "Loading Half 2 video: /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_half2_highlights.mp4\n",
            "Half 2 clip loaded: Duration 53.0s\n",
            "Concatenating 2 clips\n",
            "Writing combined video to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_combined_highlights.mp4\n",
            "Moviepy - Building video /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_combined_highlights.mp4.\n",
            "MoviePy - Writing audio in temp-audio.m4a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_combined_highlights.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_combined_highlights.mp4\n",
            "Combined highlight video saved to /content/drive/MyDrive/soccernet/highlights/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley_combined_highlights.mp4\n",
            "Highlight generation complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "import gc\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "import librosa\n",
        "\n",
        "# Constants\n",
        "FPS = 2  # Feature extraction rate\n",
        "WINDOW_SIZE = 40  # As per your training scripts\n",
        "EVENT_WINDOW = 4  # From your event detection script\n",
        "HIGHLIGHT_SECONDS_BEFORE = 5  # Seconds before event to include in highlight\n",
        "HIGHLIGHT_SECONDS_AFTER = 7   # Seconds after event to include in highlight\n",
        "MIN_IMPORTANCE_THRESHOLD = 0.25  # Minimum importance score to include in highlights\n",
        "MAX_EVENTS_PER_HALF = 5  # Limit to top 5 events per half\n",
        "\n",
        "# Event importance scores - predefined from your data\n",
        "EVENT_IMPORTANCE = {\n",
        "    \"Goal\": 0.9560,\n",
        "    \"Offside\": 0.6015,\n",
        "    \"Shot\": 0.5415,  # Shots off target\n",
        "    \"Shot on target\": 0.3765,\n",
        "    \"Foul\": 0.5302,\n",
        "    \"Penalty\": 0.4277,\n",
        "    \"Red card\": 0.2364,\n",
        "    \"Yellow->red card\": 0.2174,\n",
        "    \"Direct free-kick\": 0.1627,\n",
        "    \"Ball out of play\": 0.1019,\n",
        "    \"Yellow card\": 0.0811,\n",
        "    \"Corner\": 0.0695,\n",
        "    \"Indirect free-kick\": 0.0170,\n",
        "    \"Clearance\": 0.0127,\n",
        "    \"Kick-off\": 0.0125,\n",
        "    \"Throw-in\": 0.0087,\n",
        "    \"Substitution\": 0.0042,\n",
        "    \"Background\": 0.0\n",
        "}\n",
        "\n",
        "# Event mapper class from your event detection script\n",
        "class EventMapper:\n",
        "    def __init__(self):\n",
        "        self.events = [\n",
        "            \"Ball out of play\", \"Throw-in\", \"Foul\", \"Indirect free-kick\",\n",
        "            \"Clearance\", \"Shot\", \"Shot on target\", \"Goal\", \"Corner\", \"Substitution\",\n",
        "            \"Kick-off\", \"Yellow card\", \"Offside\", \"Direct free-kick\", \"Red card\",\n",
        "            \"Yellow->red card\", \"Penalty\", \"Background\"\n",
        "        ]\n",
        "        self.event_to_idx = {event: i for i, event in enumerate(self.events)}\n",
        "        self.idx_to_event = {i: event for i, event in enumerate(self.events)}\n",
        "\n",
        "    def get_num_classes(self):\n",
        "        return len(self.events)\n",
        "\n",
        "    def event_to_index(self, event):\n",
        "        return self.event_to_idx.get(event, self.event_to_idx[\"Background\"])\n",
        "\n",
        "    def index_to_event(self, idx):\n",
        "        return self.idx_to_event[idx]\n",
        "\n",
        "# Event Detection Model from your script\n",
        "class EventDetectionModel(nn.Module):\n",
        "    \"\"\"Advanced neural network for soccer event detection\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, dropout_rate=0.5):\n",
        "        super(EventDetectionModel, self).__init__()\n",
        "\n",
        "        # Input dimension reduction - input now includes difference features\n",
        "        self.input_dim = input_dim * 2  # Original features + temporal difference features\n",
        "\n",
        "        # Feature reduction\n",
        "        self.feature_reducer = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(WINDOW_SIZE),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # 1D convolutional layers with different kernel sizes for multi-scale temporal features\n",
        "        self.conv1 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=7, padding=3)\n",
        "\n",
        "        # Bi-directional LSTM layers\n",
        "        self.lstm1 = nn.LSTM(\n",
        "            input_size=hidden_dim * 3,  # Combined conv outputs\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Output layers with skip connection\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.classifier = nn.Linear(hidden_dim // 2, num_classes)\n",
        "\n",
        "        # Additional skip connection\n",
        "        self.skip_connection = nn.Linear(hidden_dim * 2, hidden_dim // 2)\n",
        "\n",
        "        # Batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim * 3)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim * 2)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_dim // 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, feat_dim = x.size()\n",
        "\n",
        "        # Apply feature reduction\n",
        "        orig_x = x.clone()  # Save for residual connection\n",
        "        x = x.view(-1, feat_dim)\n",
        "        x = self.feature_reducer(x.view(batch_size, seq_len, -1))\n",
        "\n",
        "        # Multi-scale temporal convolution\n",
        "        x_perm = x.permute(0, 2, 1)  # [batch, hidden_dim*2, seq_len]\n",
        "\n",
        "        conv1_out = torch.relu(self.conv1(x_perm))\n",
        "        conv2_out = torch.relu(self.conv2(x_perm))\n",
        "        conv3_out = torch.relu(self.conv3(x_perm))\n",
        "\n",
        "        # Concatenate convolutional outputs\n",
        "        conv_combined = torch.cat([conv1_out, conv2_out, conv3_out], dim=1)\n",
        "\n",
        "        # Apply batch normalization\n",
        "        conv_combined = self.bn1(conv_combined)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        lstm_in = conv_combined.permute(0, 2, 1)  # [batch, seq_len, hidden_dim*3]\n",
        "        lstm_out, _ = self.lstm1(lstm_in)  # [batch, seq_len, hidden_dim*2]\n",
        "\n",
        "        # Apply batch normalization\n",
        "        lstm_bn = self.bn2(lstm_out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attn_weights = self.attention(lstm_bn).squeeze(-1)  # [batch, seq_len]\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1).unsqueeze(-1)  # [batch, seq_len, 1]\n",
        "\n",
        "        # Context vector is weighted sum of LSTM outputs\n",
        "        context = torch.sum(lstm_bn * attn_weights, dim=1)  # [batch, hidden_dem*2]\n",
        "\n",
        "        # Feed-forward layers with skip connection\n",
        "        out1 = torch.relu(self.fc1(context))\n",
        "        out1 = self.bn3(out1)\n",
        "        out1 = nn.functional.dropout(out1, p=0.4, training=self.training)\n",
        "\n",
        "        out2 = torch.relu(self.fc2(out1))\n",
        "        out2 = self.bn4(out2)\n",
        "\n",
        "        # Skip connection\n",
        "        skip = self.skip_connection(context)\n",
        "        out = out2 + skip\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(out)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Importance Scoring Model from your first script\n",
        "class TemporalEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.conv = torch.nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.lstm = torch.nn.LSTM(hidden_dim, hidden_dim // 2, bidirectional=True, batch_first=True)\n",
        "        self.norm = torch.nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, time, dim) -> (batch, dim, time)\n",
        "        x = self.conv(x).relu()\n",
        "        x = x.permute(0, 2, 1)  # (batch, time, hidden_dim)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class HighlightModel(torch.nn.Module):\n",
        "    def __init__(self, video_dim, audio_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.video_encoder = TemporalEncoder(video_dim, hidden_dim)\n",
        "        self.audio_encoder = TemporalEncoder(audio_dim, hidden_dim)\n",
        "        self.fusion = torch.nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.scorer = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, video, audio, event_timestamps):\n",
        "        event_timestamps = event_timestamps.long()\n",
        "        video_features = self.video_encoder(video)  # (batch, time, hidden)\n",
        "        audio_features = self.audio_encoder(audio)  # (batch, time, hidden)\n",
        "        fused_features = torch.cat([video_features, audio_features], dim=-1)  # (batch, time, hidden*2)\n",
        "        fused_features = self.fusion(fused_features).relu()  # (batch, time, hidden)\n",
        "        event_features = fused_features[torch.arange(fused_features.size(0))[:, None], event_timestamps]\n",
        "        scores = self.scorer(event_features).sigmoid()  # (batch, num_events, 1)\n",
        "        return scores.squeeze(-1)  # (batch, num_events)\n",
        "\n",
        "def load_models(event_model_path, highlight_model_path, device):\n",
        "    \"\"\"Load both models from disk\"\"\"\n",
        "    event_mapper = EventMapper()\n",
        "    num_classes = event_mapper.get_num_classes()\n",
        "    event_model = EventDetectionModel(input_dim=2048, hidden_dim=256, num_classes=num_classes).to(device)\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(event_model_path, map_location=device)\n",
        "    # Extract the model state dictionary\n",
        "    event_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    event_model.eval()\n",
        "\n",
        "    highlight_model = HighlightModel(video_dim=2048, audio_dim=20, hidden_dim=512).to(device)\n",
        "    highlight_model.load_state_dict(torch.load(highlight_model_path, map_location=device))\n",
        "    highlight_model.eval()\n",
        "\n",
        "    return event_model, highlight_model, event_mapper\n",
        "\n",
        "def extract_audio_features(audio_path, target_length, feature_rate=FPS):\n",
        "    \"\"\"Extract MFCC features from audio file\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        hop_length = int(sr / feature_rate)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=hop_length)\n",
        "        audio_features = mfcc.T\n",
        "        if audio_features.shape[0] < target_length:\n",
        "            padding = np.zeros((target_length - audio_features.shape[0], audio_features.shape[1]))\n",
        "            audio_features = np.vstack([audio_features, padding])\n",
        "        elif audio_features.shape[0] > target_length:\n",
        "            audio_features = audio_features[:target_length]\n",
        "        return torch.tensor(audio_features, dtype=torch.float32), y, sr\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting audio features: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def detect_events(event_model, match_features, device):\n",
        "    \"\"\"Detect events in the match using the event detection model\"\"\"\n",
        "    event_model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(match_features) - WINDOW_SIZE + 1, 1):\n",
        "            window_features = match_features[i:i+WINDOW_SIZE]\n",
        "            # Normalize features\n",
        "            window_features = (window_features - np.mean(window_features, axis=0)) / (np.std(window_features, axis=0) + 1e-5)\n",
        "            # Add temporal difference features\n",
        "            if window_features.shape[0] > 1:\n",
        "                diff_features = np.diff(window_features, axis=0)\n",
        "                diff_features = np.vstack([np.zeros((1, window_features.shape[1])), diff_features])\n",
        "                combined_features = np.concatenate([window_features, diff_features], axis=1)\n",
        "            else:\n",
        "                combined_features = np.concatenate([window_features, np.zeros_like(window_features)], axis=1)\n",
        "\n",
        "            inputs = torch.tensor(combined_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            outputs = event_model(inputs)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            event_id = torch.argmax(outputs, dim=1).item()\n",
        "            confidence = probabilities[0, event_id].item()\n",
        "\n",
        "            frame_idx = i + WINDOW_SIZE // 2\n",
        "            time_seconds = frame_idx / FPS\n",
        "\n",
        "            results.append({\n",
        "                'frame': frame_idx,\n",
        "                'time': time_seconds,\n",
        "                'event_id': event_id,\n",
        "                'confidence': confidence\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "def filter_events(events, event_mapper, confidence_threshold=0.7, min_frame_distance=10):\n",
        "    \"\"\"Filter events - remove duplicates and low confidence predictions\"\"\"\n",
        "    events.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "    filtered_events = []\n",
        "    used_frames = set()\n",
        "\n",
        "    for event in events:\n",
        "        if event_mapper.index_to_event(event['event_id']) == \"Background\":\n",
        "            continue\n",
        "\n",
        "        if event['confidence'] < confidence_threshold:\n",
        "            continue\n",
        "\n",
        "        frame = event['frame']\n",
        "        is_nearby = any(abs(frame - used_frame) < min_frame_distance for used_frame in used_frames)\n",
        "\n",
        "        if not is_nearby:\n",
        "            used_frames.add(frame)\n",
        "            event['event_name'] = event_mapper.index_to_event(event['event_id'])\n",
        "            filtered_events.append(event)\n",
        "\n",
        "    filtered_events.sort(key=lambda x: x['frame'])\n",
        "    return filtered_events\n",
        "\n",
        "def score_events(filtered_events, highlight_model, match_video_features, match_audio_features, device):\n",
        "    \"\"\"Score events using the highlight importance model\"\"\"\n",
        "    if not filtered_events:\n",
        "        return []\n",
        "\n",
        "    highlight_model.eval()\n",
        "\n",
        "    timestamps = [event['frame'] for event in filtered_events]\n",
        "\n",
        "    video_tensor = torch.tensor(match_video_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    audio_tensor = torch.tensor(match_audio_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    timestamp_tensor = torch.tensor(timestamps, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        importance_scores = highlight_model(video_tensor, audio_tensor, timestamp_tensor)\n",
        "\n",
        "    for i, event in enumerate(filtered_events):\n",
        "        model_score = importance_scores[0, i].item()\n",
        "        event_name = event['event_name']\n",
        "        predefined_score = EVENT_IMPORTANCE.get(event_name, 0.0)\n",
        "\n",
        "        combined_score = 0.6 * model_score + 0.4 * predefined_score\n",
        "        event['model_score'] = model_score\n",
        "        event['predefined_score'] = predefined_score\n",
        "        event['combined_score'] = combined_score\n",
        "\n",
        "    filtered_events.sort(key=lambda x: x['combined_score'], reverse=True)\n",
        "    return filtered_events\n",
        "\n",
        "def generate_highlight_timestamps(scored_events, max_highlight_duration=180, max_events=MAX_EVENTS_PER_HALF):\n",
        "    \"\"\"Generate timestamps for the highlight video, limited to max_events\"\"\"\n",
        "    print(\"Generating highlight timestamps...\")\n",
        "    # Sort by score to select top events\n",
        "    scored_events = sorted(scored_events, key=lambda x: x['combined_score'], reverse=True)[:max_events]\n",
        "    # Sort by time for chronological order\n",
        "    scored_events.sort(key=lambda x: x['time'])\n",
        "\n",
        "    highlight_segments = []\n",
        "    total_duration = 0\n",
        "\n",
        "    for event in scored_events:\n",
        "        print(f\"Evaluating event: {event['event_name']} at {event['time']:.1f}s, \"\n",
        "              f\"Combined Score: {event['combined_score']:.4f}, \"\n",
        "              f\"Model Score: {event['model_score']:.4f}, \"\n",
        "              f\"Predefined Score: {event['predefined_score']:.4f}\")\n",
        "\n",
        "        if event['combined_score'] < MIN_IMPORTANCE_THRESHOLD:\n",
        "            print(f\"Skipping event: Score {event['combined_score']:.4f} below threshold {MIN_IMPORTANCE_THRESHOLD}\")\n",
        "            continue\n",
        "\n",
        "        start_time = max(0, event['time'] - HIGHLIGHT_SECONDS_BEFORE)\n",
        "        end_time = event['time'] + HIGHLIGHT_SECONDS_AFTER\n",
        "        segment_duration = end_time - start_time\n",
        "\n",
        "        if total_duration + segment_duration > max_highlight_duration:\n",
        "            print(f\"Skipping event: Adding {segment_duration}s would exceed max duration \"\n",
        "                  f\"(Current: {total_duration}s, Max: {max_highlight_duration}s)\")\n",
        "            if len(highlight_segments) > 0:\n",
        "                continue\n",
        "\n",
        "        highlight_segments.append({\n",
        "            'start': start_time,\n",
        "            'end': end_time,\n",
        "            'event': event['event_name'],\n",
        "            'score': event['combined_score']\n",
        "        })\n",
        "        total_duration += segment_duration\n",
        "        print(f\"Added segment: {event['event_name']} from {start_time}s to {end_time}s, \"\n",
        "              f\"Duration: {segment_duration}s, Total Duration: {total_duration}s\")\n",
        "\n",
        "        if total_duration >= max_highlight_duration:\n",
        "            print(\"Max duration reached, stopping segment generation\")\n",
        "            break\n",
        "\n",
        "    if highlight_segments:\n",
        "        print(\"Merging overlapping segments...\")\n",
        "        merged_segments = [highlight_segments[0]]\n",
        "        for segment in highlight_segments[1:]:\n",
        "            prev = merged_segments[-1]\n",
        "            if segment['start'] <= prev['end']:\n",
        "                prev['end'] = max(prev['end'], segment['end'])\n",
        "                if segment['score'] > prev['score']:\n",
        "                    prev['event'] = segment['event']\n",
        "                    prev['score'] = segment['score']\n",
        "                print(f\"Merged segment: {prev['event']} from {prev['start']}s to {prev['end']}s\")\n",
        "            else:\n",
        "                merged_segments.append(segment)\n",
        "                print(f\"Added non-overlapping segment: {segment['event']} from {segment['start']}s to {segment['end']}s\")\n",
        "\n",
        "        print(f\"Generated {len(merged_segments)} highlight segments\")\n",
        "        return merged_segments\n",
        "\n",
        "    print(\"No highlight segments generated\")\n",
        "    return []\n",
        "\n",
        "def create_highlight_video(video_path, highlight_segments, output_path):\n",
        "    \"\"\"Create the highlight video by extracting and concatenating clips\"\"\"\n",
        "    print(f\"Attempting to create highlight video: {output_path}\")\n",
        "    print(f\"Input video path: {video_path}\")\n",
        "    print(f\"Highlight segments: {highlight_segments}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(video_path):\n",
        "            print(f\"Video file not found: {video_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading video file: {video_path}\")\n",
        "        video = VideoFileClip(video_path)\n",
        "        print(f\"Video duration: {video.duration}s, FPS: {video.fps}\")\n",
        "\n",
        "        clips = []\n",
        "        for segment in highlight_segments:\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            print(f\"Processing segment: {segment['event']} from {start_time}s to {end_time}s\")\n",
        "\n",
        "            start_time = max(0, min(start_time, video.duration))\n",
        "            end_time = max(start_time, min(end_time, video.duration))\n",
        "\n",
        "            if end_time <= start_time:\n",
        "                print(f\"Skipping invalid segment: start_time ({start_time}s) >= end_time ({end_time}s)\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                print(f\"Extracting subclip from {start_time}s to {end_time}s\")\n",
        "                clip = video.subclip(start_time, end_time)\n",
        "                clips.append(clip)\n",
        "                print(f\"Added clip of duration {clip.duration}s\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting subclip from {start_time}s to {end_time}s: {e}\")\n",
        "\n",
        "        if clips:\n",
        "            print(f\"Concatenating {len(clips)} clips\")\n",
        "            final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
        "            print(f\"Writing output video to {output_path}\")\n",
        "            final_clip.write_videofile(\n",
        "                output_path,\n",
        "                codec=\"libx264\",\n",
        "                audio_codec=\"aac\",\n",
        "                temp_audiofile=\"temp-audio.m4a\",\n",
        "                remove_temp=True,\n",
        "                verbose=True\n",
        "            )\n",
        "            print(f\"Highlight video successfully saved to {output_path}\")\n",
        "        else:\n",
        "            print(\"No valid clips to concatenate!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in create_highlight_video: {e}\")\n",
        "    finally:\n",
        "        if 'video' in locals():\n",
        "            video.close()\n",
        "        if 'final_clip' in locals():\n",
        "            final_clip.close()\n",
        "        if 'clips' in locals():\n",
        "            for clip in clips:\n",
        "                if clip:\n",
        "                    clip.close()\n",
        "\n",
        "def combine_highlight_videos(half1_video_path, half2_video_path, output_path):\n",
        "    \"\"\"Combine two highlight videos into a single video\"\"\"\n",
        "    print(f\"Attempting to combine highlight videos into: {output_path}\")\n",
        "    print(f\"Half 1 video: {half1_video_path}\")\n",
        "    print(f\"Half 2 video: {half2_video_path}\")\n",
        "\n",
        "    try:\n",
        "        clips = []\n",
        "        if os.path.exists(half1_video_path):\n",
        "            print(f\"Loading Half 1 video: {half1_video_path}\")\n",
        "            clip1 = VideoFileClip(half1_video_path)\n",
        "            clips.append(clip1)\n",
        "            print(f\"Half 1 clip loaded: Duration {clip1.duration}s\")\n",
        "        else:\n",
        "            print(f\"Half 1 video not found: {half1_video_path}\")\n",
        "\n",
        "        if os.path.exists(half2_video_path):\n",
        "            print(f\"Loading Half 2 video: {half2_video_path}\")\n",
        "            clip2 = VideoFileClip(half2_video_path)\n",
        "            clips.append(clip2)\n",
        "            print(f\"Half 2 clip loaded: Duration {clip2.duration}s\")\n",
        "        else:\n",
        "            print(f\"Half 2 video not found: {half2_video_path}\")\n",
        "\n",
        "        if clips:\n",
        "            print(f\"Concatenating {len(clips)} clips\")\n",
        "            final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
        "            print(f\"Writing combined video to {output_path}\")\n",
        "            final_clip.write_videofile(\n",
        "                output_path,\n",
        "                codec=\"libx264\",\n",
        "                audio_codec=\"aac\",\n",
        "                temp_audiofile=\"temp-audio.m4a\",\n",
        "                remove_temp=True,\n",
        "                verbose=True\n",
        "            )\n",
        "            print(f\"Combined highlight video saved to {output_path}\")\n",
        "\n",
        "            final_clip.close()\n",
        "            for clip in clips:\n",
        "                clip.close()\n",
        "        else:\n",
        "            print(\"No valid clips to concatenate for combined video!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in combine_highlight_videos: {e}\")\n",
        "    finally:\n",
        "        if 'final_clip' in locals():\n",
        "            final_clip.close()\n",
        "        if 'clips' in locals():\n",
        "            for clip in clips:\n",
        "                if clip:\n",
        "                    clip.close()\n",
        "\n",
        "def generate_highlights(match_dir, event_model, highlight_model, event_mapper, output_dir, device):\n",
        "    \"\"\"Main function to generate highlights for a match\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    half1_features_path = os.path.join(match_dir, \"1_ResNET_TF2.npy\")\n",
        "    half2_features_path = os.path.join(match_dir, \"2_ResNET_TF2.npy\")\n",
        "\n",
        "    if not os.path.exists(half1_features_path) or not os.path.exists(half2_features_path):\n",
        "        print(f\"Error: Feature files not found in {match_dir}\")\n",
        "        return\n",
        "\n",
        "    half1_features = np.load(half1_features_path)\n",
        "    half2_features = np.load(half2_features_path)\n",
        "\n",
        "    half1_video_path = os.path.join(match_dir, \"1_224p.mkv\")\n",
        "    half2_video_path = os.path.join(match_dir, \"2_224p.mkv\")\n",
        "\n",
        "    if not os.path.exists(half1_video_path):\n",
        "        print(f\"Warning: Half 1 video file not found: {half1_video_path}\")\n",
        "    if not os.path.exists(half2_video_path):\n",
        "        print(f\"Warning: Half 2 video file not found: {half2_video_path}\")\n",
        "\n",
        "    half1_audio_path = os.path.join(match_dir, \"1_224p.wav\")\n",
        "    half2_audio_path = os.path.join(match_dir, \"2_224p.wav\")\n",
        "\n",
        "    match_name = os.path.basename(match_dir)\n",
        "\n",
        "    all_highlight_segments = []\n",
        "\n",
        "    for half_idx, (features, video_path, audio_path) in enumerate(\n",
        "        [(half1_features, half1_video_path, half1_audio_path),\n",
        "         (half2_features, half2_video_path, half2_audio_path)],\n",
        "        start=1):\n",
        "\n",
        "        print(f\"\\nProcessing half {half_idx} of {match_name}...\")\n",
        "\n",
        "        if os.path.exists(audio_path):\n",
        "            audio_features, audio_signal, sr = extract_audio_features(audio_path, features.shape[0])\n",
        "            if audio_features is None:\n",
        "                print(f\"Failed to extract audio features from {audio_path}\")\n",
        "                audio_features = torch.zeros(features.shape[0], 20)\n",
        "        else:\n",
        "            print(f\"Audio file not found: {audio_path}, using zeros for audio features\")\n",
        "            audio_features = torch.zeros(features.shape[0], 20)\n",
        "            audio_signal, sr = None, None\n",
        "\n",
        "        print(\"Detecting events...\")\n",
        "        events = detect_events(event_model, features, device)\n",
        "\n",
        "        print(\"Filtering events...\")\n",
        "        filtered_events = filter_events(events, event_mapper)\n",
        "\n",
        "        if not filtered_events:\n",
        "            print(f\"No events detected in half {half_idx}\")\n",
        "            continue\n",
        "\n",
        "        print(\"Scoring events...\")\n",
        "        scored_events = score_events(filtered_events, highlight_model, features, audio_features, device)\n",
        "\n",
        "        print(\"Generating highlight timestamps...\")\n",
        "        highlight_segments = generate_highlight_timestamps(scored_events)\n",
        "        print(f\"Generated {len(highlight_segments)} highlight segments\")\n",
        "\n",
        "        for segment in highlight_segments:\n",
        "            all_highlight_segments.append({\n",
        "                'half': half_idx,\n",
        "                'video_path': video_path,\n",
        "                'start': segment['start'],\n",
        "                'end': segment['end'],\n",
        "                'event': segment['event'],\n",
        "                'score': segment['score']\n",
        "            })\n",
        "\n",
        "        print(f\"\\nDetected {len(scored_events)} events in half {half_idx}:\")\n",
        "        for i, event in enumerate(scored_events[:10]):\n",
        "            print(f\"{i+1}. {event['event_name']} at {event['time']:.1f}s, Score: {event['combined_score']:.4f}\")\n",
        "\n",
        "        if video_path and os.path.exists(video_path) and highlight_segments:\n",
        "            output_path = os.path.join(output_dir, f\"{match_name}_half{half_idx}_highlights.mp4\")\n",
        "            print(f\"\\nCreating highlight video for half {half_idx}...\")\n",
        "            create_highlight_video(video_path, highlight_segments, output_path)\n",
        "        else:\n",
        "            print(f\"Skipping video creation for half {half_idx}: Video path valid={os.path.exists(video_path)}, Segments={len(highlight_segments)}\")\n",
        "\n",
        "        highlight_json_path = os.path.join(output_dir, f\"{match_name}_half{half_idx}_highlights.json\")\n",
        "        with open(highlight_json_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'match': match_name,\n",
        "                'half': half_idx,\n",
        "                'events': scored_events,\n",
        "                'highlight_segments': highlight_segments\n",
        "            }, f, indent=2)\n",
        "        print(f\"Highlight data saved to {highlight_json_path}\")\n",
        "\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        for event in scored_events:\n",
        "            plt.axvline(x=event['time'], color='r', alpha=0.3, linestyle='--')\n",
        "            plt.text(event['time'], 1.0, event['event_name'], rotation=90, alpha=0.7)\n",
        "\n",
        "        for segment in highlight_segments:\n",
        "            plt.axvspan(segment['start'], segment['end'], alpha=0.2, color='green')\n",
        "\n",
        "        plt.title(f\"Highlights for {match_name} - Half {half_idx}\")\n",
        "        plt.xlabel(\"Time (seconds)\")\n",
        "        plt.ylabel(\"Event Importance\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(0, features.shape[0] / FPS)\n",
        "        vis_path = os.path.join(output_dir, f\"{match_name}_half{half_idx}_highlights_vis.png\")\n",
        "        plt.savefig(vis_path)\n",
        "        plt.close()\n",
        "\n",
        "    # Combine existing highlight videos\n",
        "    half1_highlight_path = os.path.join(output_dir, f\"{match_name}_half1_highlights.mp4\")\n",
        "    half2_highlight_path = os.path.join(output_dir, f\"{match_name}_half2_highlights.mp4\")\n",
        "    combined_output_path = os.path.join(output_dir, f\"{match_name}_combined_highlights.mp4\")\n",
        "    combine_highlight_videos(half1_highlight_path, half2_highlight_path, combined_output_path)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main entry point\"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Drive mounting issue (may already be mounted): {e}\")\n",
        "\n",
        "    event_model_path = '/content/drive/MyDrive/soccernet/soccernet_event_detection_model.pth'\n",
        "    highlight_model_path = '/content/drive/MyDrive/soccernet/highlight_model.pth'\n",
        "\n",
        "    match_dir = '/content/drive/MyDrive/soccernet/england_epl/2014-2015/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley'\n",
        "    output_dir = '/content/drive/MyDrive/soccernet/highlights'\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading models...\")\n",
        "    event_model, highlight_model, event_mapper = load_models(event_model_path, highlight_model_path, device)\n",
        "\n",
        "    generate_highlights(match_dir, event_model, highlight_model, event_mapper, output_dir, device)\n",
        "\n",
        "    print(\"Highlight generation complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}